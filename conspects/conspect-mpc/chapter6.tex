\section{Explicit MPC}

Idea: Compute an "explicit MPC control law" by solving the optimization problem for all x.

Linear discrete-time systems $x^+=Ax+Bu$

Polytopic input+state constraints $C_xx \leq d_x$,  $C_uu \leq d_u$

MPC problem. At time $t$, given $x(t)$, solve
\begin{equation*}
\min_{u(\cdot|t)} = \sum_{k=t}^{t+N-1}L(x(k|t),u(k|t)) + F(x(t+N|t))
\end{equation*}
s.t.
\begin{equation*}
x(k+1|t) = Ax(k|t) + Bu(k|t)
\end{equation*}
\begin{equation*}
x(t|t) = x(t)
\end{equation*}
\begin{equation*}
C_xx(k|t) \leq d_x
\end{equation*}
\begin{equation*}
C_uu(k|t) \leq d_u
\end{equation*}
for $ t \leq k \leq t+N-1$ and with terminal constraint
\begin{equation*}
C^fx(t+N|t) \leq d^f
\end{equation*}
with $L(x,u) = x^TQx + u^TRu, \ Q,R > 0, \ F(x) = x^TPx$

$\Rightarrow$ optimizer: $u^*(\cdot|t)$

Define: 
\begin{equation}
X:= [x^T(t+1|t), \dots, x^T(t+N|t)]^T
\end{equation}
\begin{equation}
U:= [u^T(t+1|t), \dots, u^T(t+N-1|t)]^T
\end{equation}
\begin{itemize}
\item rewrite cost function
\begin{equation}\label{cost_function}
F(x(t),U) = x^T(t)Qx(t) + X^T\tilde{Q}X + U^T\tilde{R}U
\end{equation}
with $\tilde{Q} = 
\begin{bmatrix}
    Q  & \  & \ \\
    \  &\ddots & \\
     \  & \ & Q \\
     \ & \ & \ & P     
\end{bmatrix} $, 
$\tilde{R} = 
\begin{bmatrix}
    R  & \  & \ \\
    \  &\ddots & \\
     \  & \ & R     
\end{bmatrix} $
\item rewrite predicted states: $x(t+k|t) = A^kx(t) + \sum_{j=0}^{k-1}A^jBu(t+k-j-1|t), \ k=1,..,N$
\begin{equation}\label{rewritten_state_input}
\Rightarrow X=\begin{bmatrix}
   A \\
   A^2 \\
   \vdots \\
   A^N   
\end{bmatrix}x(t) +\begin{bmatrix}
    B  & 0 & \dots &  & 0 \\
    AB & B & 0 & \dots & 0\\
    \hdots & & & & \\
    A^{N-1}B  & A^{N-2}B & \dots & \dots & B     
\end{bmatrix}U
\end{equation}
where $\Omega = \begin{bmatrix}
   A \\
   A^2 \\
   \vdots \\
   A^N   
\end{bmatrix}$ and $\Gamma = \begin{bmatrix}
    B  & 0 & \dots &  & 0 \\
    AB & B & 0 & \dots & 0\\
    \hdots & & & & \\
    A^{N-1}B  & A^{N-2}B & \dots & \dots & B     
\end{bmatrix}$

Plugging (\ref{rewritten_state_input}) in (\ref{cost_function}) : $J(x(t),U) = \frac{1}{2}x^T(t)Yx(t) + \frac{1}{2}U^THU + x^T(t)FU$
with
\begin{equation*}
Y = 2(Q + \Omega^T \tilde{Q} \Omega)
\end{equation*}
\begin{equation*}
H = 2(\Gamma^T\tilde{Q}\Gamma + \tilde{R})
\end{equation*}
\begin{equation*}
F = 2\Omega^T \tilde{Q} \Gamma
\end{equation*}

\item rewrite constraints: using (\ref{rewritten_state_input})
\begin{equation*}
\begin{bmatrix}
    C_x  & \  & \ \\
    \  &\ddots & \\
     \  & \ & C_x     
\end{bmatrix} (\Omega x(t) + \Gamma U) \leq \begin{bmatrix}
   d_x \\
   \vdots \\
   d_x   
\end{bmatrix}
\end{equation*}
$\Rightarrow$ constraints in total
\begin{equation*}
GU \leq W + Ex(t)
\end{equation*}
\end{itemize}

\begin{equation}\label{standard_problem}
\min_U \frac{1}{2} U^THU + x^TFU + \frac{1}{2}x^TYx
\end{equation}
s.t. $GU \leq W + Ex$

multiparametric quadratic program (MPQP)

Last step: define $z:=U+H^{-1}F^Tx$, $H^{-1}$ - pos.definite 

\begin{equation}\label{rewritten_problem}
\min_{z} \frac{1}{2}z^THz + \frac{1}{2}x^T \tilde{Y}x
\end{equation}
s.t. $Gz \leq W + Sx$
\begin{equation*}
\tilde{Y} := Y - FH^{-1}F^T
\end{equation*}
\begin{equation*}
S := E + GH^{-1}F^T
\end{equation*}

This is a (strictly) convex optimization problem with (by assumption) feasible set with non-empty interior $\leadsto$ Slater's condition satisfied

$\Rightarrow$ Optimal solution (unique!) is characterized by following KKT conditions

\begin{equation}\label{condition_a}
Hz + G^T\lambda = 0, \ \lambda \in \mathbb{R}^q
\end{equation}

$\triangledown$(cost + $\lambda^T$constraints) = 0, complementary slackness condition below
\begin{equation}\label{condition_b}
\lambda^i(G^iz - W^i-S^ix) = 0 \ i=1, ...q
\end{equation}

$i$ - th component of vector $\lambda$ or $G$

\begin{equation}\label{condition_c}
\lambda \geq 0
\end{equation}
\begin{equation}\label{condition_d}
Gz \leq W + Sx
\end{equation}

image to be inserted

\begin{Definition}
optimal active set $z^*(x) \dots$ optimal solution to (\ref{rewritten_problem}) for a given $x$
\begin{equation*}
A(x) := \{ i \in \{1, \dots q\} | G^iz^*(x) = W^i + S^ix \}
\end{equation*}

$\leadsto G^A, \ W^A, \ S^A \dots$ matricies containing rows of $G, \ U, \ S$ associated to indices in $A$ 
\end{Definition}

Assumption: Linear independence constraint qualification (LICQ) $G^A$ has full row rank ("gradient of active constraints are linearly independent")

From (\ref{condition_a}) 
\begin{equation}\label{active_constraint}
\leadsto z^*(x) = -H^{-1}G^T\lambda = -H^{-1}(G^A)^T\lambda^A
\end{equation}

For all active constraints, we have $G^Az(x) = W^A + S^Ax \to^{(\ref{active_constraint})} - G^AH^{-1}(G^A)^T\lambda^A = W^A+S^Ax$

\begin{equation}\label{lambda_equation}
\lambda^A = - (G^AH^{-1}(G^A)^T)^{-1}(W^A+S^Ax)
\end{equation} 

Plug (\ref{lambda_equation}) into (\ref{active_constraint}):
\begin{equation}\label{optimal_z}
z^*(x) = H^{-1}(G^A)^T(G^AH^{-1}(G^A)^T)^{-1}(W^A + S^Ax)
\end{equation}

For a given active constraint set, optimal solution is an affine function of $x$.

Characterization of the set where $A$ is an optimal active set (critical region $CR^A$)

Use (\ref{condition_c}), (\ref{condition_d}): Plug (\ref{optimal_z}) into (\ref{condition_d})
\begin{equation}\label{inequality_with_z}
GH^{-1}(G^A)^T(G^AH^{-1}(G^A)^T)^{-1}(W^A+S^Ax) \leq W + Sx
\end{equation}

Plug (\ref{lambda_equation}) into (\ref{condition_c})
\begin{equation}\label{c_condition_with_lambda}
-(G^AH^{-1}(G^A)^T)^{-1}(W^A+S^Ax) \geq 0
\end{equation}

(\ref{inequality_with_z}) + (\ref{c_condition_with_lambda}) describe critical region $CR^A$, on which the optimal solution is given by (\ref{lambda_equation}) -(\ref{optimal_z})

image to be inserted

Algorithm:
\begin{itemize}
\item Take some $x_0 \in X$ (e.g., $x_0 = 0$)
\item Solve (\ref{rewritten_problem}) with $x = x_0 \leadsto z*(x_0)$
\item Identify active constraints $\leadsto G^A, W^A, S^A$
\item Calculate the corresponding critical region $CR^A$ via (\ref{inequality_with_z}) and (\ref{c_condition_with_lambda})
$\Rightarrow$ inside this critical region, solution to (\ref{rewritten_problem}) is given by (\ref{optimal_z})
\item Step over all facets of this first critical region, take new $x_0$ go to (\ref{rewritten_state_input})
\end{itemize}  

What happens if more than two critical regions are adjacent to a critical region?

Procedure needs to be suitably adapted in case of degenerate problems (LICQ assumption cost satisfied)

Can merge critical regions if union is convex and the resulting control is the same

Main bottleneck: number of regions can grow very quickly
\begin{itemize}
\item online point-location problem becomes complex
\end{itemize}

\begin{Theorem}
For linear MPC (linear system, linear constraints, quadratic cost), the resulting MPC positive definite controller $u_{MPC}(x)$ is continuous and piecewise affine over polyhedral regions. The optimal value function to (\ref{standard_problem}), $F^*(x)$, is continuous, convex and piecewise quadratic.
\begin{proof}
\begin{itemize}
\item $u_{MPC}(x)$ is affine by (\ref{optimal_z})
\item since solution is unique and a point on the boundary of two regions belongs to both regions, the two control laws must be equal on the boundary $\Rightarrow u_{MPC}$ - continuous
\item optimal value function of (\ref{rewritten_problem})
\begin{equation*}
J_z^*(x) = \frac{1}{2}z^*(x)^THz^*(x) 
\end{equation*} 
continuous and piecewise quadratic
$\leadsto$ can show that it is convex (by standard arguments) $\leadsto u^*(x) = z^*(x) - H^{-1}F^Tx$
\begin{equation*}
J^*(x) = \frac{1}{2}U^*(x)^THU^*(x) + x^TFU^*(x) + \frac{1}{2}x^TYx = \dots = J_z^*(x) + \frac{1}{2}x^T(Y-FH^{-1}F^T)x
\end{equation*}
First term is convex, continuous piecewise quadratic, the second term is continuous quadratic + convex.

Why convex?
$\begin{bmatrix}
Y & F^T \\
F & H
\end{bmatrix} \geq 0$ as $J(x,u(\cdot)) \geq 0 \leadsto$ Schur complement implies $Y-FH^{-1}F^T \geq 0$ 
\end{itemize}
\end{proof}
\end{Theorem}

\section{part of economic mpc}
Closed-loop convergence to the optimal steady state

\begin{Theorem}
Suppose the system is strictly dissipative w.r.t. the supply rate $s(x,u) = L(x,u) - L(x_s, u_s)$. Then the closed-loop system asymptotically converge to the optimal steady state $x_s$.

Remark: $x_s$ is asymptotically stable if $J^*(x)$ and $\lambda(x)$ are continuous at $x_s$.
\begin{proof}
Define "rotated" cost function
\begin{equation*}
\tilde{L}(x,u) =L(x,u) - L(x_s, u_s) + \lambda(x) - \lambda(f(x,u))
\end{equation*}

Auxiliary optimization problem $\tilde{P}$
\begin{equation*}
\min_{u(\cdot| t)} \tilde{J}(x(t),u(t)) = \sum_{k=t}^{t+N-1}\tilde{L}(x(k|t),u(k|t))
\end{equation*}

s.t. some constraints as in original MPC problem

Claim: $P$ and $\tilde{P}$ have the same minimizer. 
Proof of claim: feasible sets coincide 
\begin{equation*}
\tilde{J}(x(t),u(t)) = \sum_{k=t}^{t+N-1} [L(x(k|t),u(k|t)) + \lambda(x(k|t)) - \lambda(f(x(k|t),u(k|t))) - L(x_s,u_s)] = \lambda(x(t|t)) - \lambda(x(t+N|t)) - NL(x_s,u_s) + \sum_{k=1}^{t+N-1}L(x(k|t),u(k|t)) = \lambda(x(t)) - \lambda(x_s) - NL(x_s,u_s) + J(x(t),u(\cdot | t))
\end{equation*}
$\Rightarrow \tilde{J}$ and $J$ only differ by a constant $\Rightarrow P$ and $\tilde{P}$ have the same minimizer.

$\Rightarrow$ we can use $\tilde{P}$ to analyze stability of the closed loop. $\tilde{L}(x,u) \geq \rho (|x - x_s|)$, $\tilde{L}$ is positive definite (from strictly dissipative) $\Rightarrow$ can apply standard MPC stability results.  
\end{proof}
\end{Theorem}

Average constraints:

\begin{Definition}
Asymptotic average constraints $h: \mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^p$ - (auxiliary) output specifying average constraints.
\end{Definition}

Assumption: $h$ is continuous
\begin{itemize}
\item $Av[h(x,u)] \in Y$
\item $Av[h(x,u)] := {\bar{h}: \exists {t_k} \to \infty \ s.t. \ \lim_{n \to \infty} \frac{\sum_{t=0}^{t_n - 1}h(x(t),u(t))}{t_n}} = \bar{h}$

$Av[h(x,u)]$ "set of asymptotic average values of $h$".
\end{itemize}

How to modify the MPC problem accordingly?

Adaptation of optimal steady state computation
\begin{equation*}
(x_s,u_s) = argmin_{x = f(x,u), x \in X, u \in U, h(x,u) \in Y} L(x,u)
\end{equation*}

Additional constraint:
\begin{equation*}
\sum_{k=t}^{t+N-1}h(x(k|t),u(k|t)) \in Y_t
\end{equation*}
with $Y_{t+1} := Y_t \oplus Y \oplus {-h(x(t),u(t))}$, $Y_0 = NY \oplus Y_{\infty}$, $Y_{\infty}$ - arbitrary compact set.

\begin{enumerate}
\item How to guarantee recursive feasibility?

Candidate solution at time $t+1$ 
\begin{equation*}
\tilde{u}(k|t+1) = \left \{
  \begin{tabular}{ll}
  $u^*(k|t)$ & $\ t+1 \leq k \leq t+N+1$ \\
  $u_s$ & $\ k = t + N$
  \end{tabular}
\right .
\end{equation*}
\begin{equation*}
\sum_{k=t+1}^{t+N}h(\tilde{x}(k|t+1), \tilde{u}(k|t+1)) = \sum_{k=t}^{t+k-1}h(x^*(k|t), u^*(k|t))[\in Y_t] - h(x(t),u(t)) + h(x_s,u_s)[\in Y]
\end{equation*}
$\in Y_t \oplus Y \oplus {-h(x(t),u(t))} = Y_{t+1}$
\item Satisfaction of average constraint for the resulting closed loop:

Assumption: $Y$ is convex. Solve the recursion for $Y_t$ to obtain 
\begin{equation*}
Y_t = Y_{\infty} \oplus (t+N)Y \oplus {- \sum_{k=0}^{t-1}h(x(k),u(k))}
\end{equation*}
the last item is closed loop sequence.

From the additional constraint in optimal problem:
\begin{equation*}
\sum_{k=0}^{t-1}h(x(k),u(k)) + \sum_{k=t}^{t+N-1}h(x(k|t), u(k|t)) \in Y_{\infty} \oplus (t+N)Y
\end{equation*}

For each sequence {$t_n$} s.t. $\lim_{n \to \infty} \sum_{k=0}^{t_n -1}\frac{h(x(k),u(k))}{t_n} $ exists, we obtain 
\begin{equation*}
\lim_{n \to \infty} \frac{\sum_{k=0}^{t_n-1}h(x(k),u(k))}{t_n} \in \lim_{n \to \infty} \frac{Y_{\infty} \oplus (t_n + N)Y}{t_n} = Y
\end{equation*}
$\Rightarrow$ closed-loop system satisfies average constraints.
\end{enumerate}

Transient average constraint:
\begin{equation*}
\sum_{k=t}^{t+T-1}\frac{h(x(k),u(k))}{T} \in Y, \ \forall t \geq 0
\end{equation*}
for $T=1$, standard pointwise-in-time 

Constraints are recovered

Assumption: $Y= \mathbb{R}^p_{\leq 0}$

%TODO picture

$T+N-1$ constraints in total,described below
\begin{equation*}
\left \{
\begin{tabular}{c} 
$\sum_{t-T+\tau}^{t-1} h(x(k),u(k)) + \sum_{i=t}^{t+\tau -1}h(x(i|t), u(i|t)) \geq 0 \ \tau = 1, ..., T$ \\
$\sum_{i=t+j+1}^{t+T+j}h(x(i|t),u(i|t)) \leq 0, \ j =0, ..., N-2$ \\
$u(i|t) = u_s \ i = t+N, ... , t+N-2+T$ 
\end{tabular} 
\right .  
\end{equation*}

\begin{itemize}
\item recursive feasibility can be shown using standard candidate sequence
\item closed-loop transient average constraint satisfaction directly follows from first additional constraint
\end{itemize}
