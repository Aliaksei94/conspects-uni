\section{Backstepping}

Integrator backstepping 
\begin{equation}\label{system_backstepping}
\dot x_1 = f_1(x_1) + g_1(x_1)x_2 
\end{equation}
\begin{equation*}
\dot x_2 = u
\end{equation*}

where $x_1 \in \mathbb{R}^m, \ x_2, \ u \in \mathbb{R}$ (single input)

image to be inserted

Assumption: we know (smooth) "feedback" $\alpha_1: \mathbb{R}^n \to \mathbb{R}$, and positive definite, differentiable $v_1: \mathbb{R}^m \to \mathbb{R}$

s.t. $L_{f_1 + g_1\alpha_1}V_1(x)$ is negative definite $\Rightarrow$ origin of $\dot{x_1} = f_1(x_1) + g_1(x_1)\alpha_1(x_1)$ is asymptotically stable

Goal: Compute feedback $u = k(x)$ which stabilises (\ref{system_backstepping}). Backstepping constructs $u = \alpha_2(x_1, x_2)$ s.t. $(e_1, e_2) = (x_1 - 0, x_2 - \alpha_1(x_1))=0$ error coordinates

Rewrite (\ref{system_backstepping}) :
\begin{equation*}
\dot x_1 = f_1(x_1) + g_1\alpha_1(x_1) + g_1(x_1)(x_2 - \alpha_1(x_1))
\end{equation*}  
\begin{equation*}
\dot x_2 = u
\end{equation*}

image to be inserted

In error coordinates

\begin{equation}\label{error_coordinates_rewriting}
\dot e_1 = f_1(e_1) + g_1(e_1)\alpha_1(e_1) +g_1(e_1)e_2
\end{equation}
\begin{equation*}
\dot e_2 = u - \dot{\alpha_1} = u - \frac{\partial \alpha_1}{\partial e_1}\dot{e_1} = u - \frac{\partial \alpha_1}{\partial e_1}
\end{equation*}

"backstepping" $\alpha_1$ through the integrator

Define $V_2(e_2):= \frac{1}{2} e_2^2$, and 
\begin{equation*}
V(e_1, e_2) = V_1(e_1) + V_2(e_2)
\end{equation*}  
\begin{equation*}
\dot V(e_1, e_2) = \frac{\partial V_1}{\partial e_1}(f_1(e_1) + g_1(e_1)\alpha_1(e_1)) + \frac{\partial V_1}{\partial e_1}g_1(e_1)e_2 + \frac{\partial V_2}{\partial e_2}(u - \dot{\alpha_1})
\end{equation*}

as far as $L_{f_1 + g_1\alpha_1}V_1$ -negative definite and $\frac{\partial V_2}{\partial e_2} \rightarrow e_2$

Choose 
\begin{equation}\label{u_choice}
u = (- \frac{\partial V_1}{\partial e_1}g_1(e_1) + \dot{\alpha_1})("canaling\ terms") - k_2e_2 ("stabilizing\ term") k_2 > 0
\end{equation}

$\Rightarrow$ Then $\dot{V}(e_1, e_2) = L_{f_1 + g_1\alpha_1}V_1(e_1) - k_2e_2^2 < 0, \ \forall (e_1, e_2) \neq 0$

$\Rightarrow$ Then $\dot V (e_1, e_2) = L_{f_1 + g_1\alpha_1}V_1(e_1) - k_2e_2^2 < 0 \ \forall (e_1, e_2) \neq 0$

$\Rightarrow (e_1, e_2) = (0,0)$ is an asymptotically stable EP for (\ref{error_coordinates_rewriting}) with $u$ as in (\ref{u_choice}) 

Remark: $(e_1, e_2) \to (0,0)$ doesnot necessarily imply that $(x_1, x_2) \to 0$ for $u = \alpha_2(x_1, x_2) = - \frac{\partial V_1}{\partial x_1}g_1(x_1) + \dot{\alpha_1} - k_2(x_2 - \alpha_1(x_1))$

where $u \leftarrow$ (\ref{u_choice}) the original coordinates and $\dot{\alpha_1} \leftarrow \frac{\partial \alpha_1}{\partial x_1}(f_1(x_1) + g_1(x_1)x_2)$

But $(x_1, x_2) = (0,0)$ is asymptotically stable if $\alpha_1(0) = 0$ why? $(e_1,e_2) \to 0 \Rightarrow x_1 \to 0 \ x_2 \to \alpha_1(0) = 0$

\begin{Example}
\begin{equation*}
\dot{x_1} = x_1x_2
\end{equation*}
\begin{equation*}
\dot{x_2} = u
\end{equation*}

Choose $\alpha_1(x_1) = -k$ ($k > 0$) $\rightarrow \dot{x_1} = -k x_1 \Rightarrow V_1(x_1) = \frac{1}{2}x_1^2$

Then: 
\begin{equation*}
e_1 = x_1 - 0 \ \dot{e_1} = e_1(e_2 - k)
\end{equation*}
\begin{equation*}
e_2 = x_2 + k \ e_2 = u
\end{equation*}

Backstepping yields: $u = - e_1^2 - k_2e_2 \ k_2 > 0 \Rightarrow (e_1,e_2) = (0,0)$ is asymptotically stabilized

$(x_1, x_2) = (0,-k)$ is asymptotically stabilized

Can we choose different $\alpha_1$ s.t. $(x_1, x_2) = (0,0)$ is stabilized?

Yes, e.g.
\begin{equation*}
\alpha_1(x_1) = -x_1^2 \Rightarrow \dot{x_1} = - x_1^3 \ V_1(x_1) = \frac{1}{2}x_1^2
\end{equation*}

So we have equations

\begin{equation*}
e_1 = x_1 - 0 \ \dot{e_1}  = e_1(e_2 - e_1^2) 
\end{equation*}
\begin{equation*}
e_2 = x_2 + x_1^2 \ \dot{e_2} = u + 2e_1^2(e_2 - e_1^2)
\end{equation*}

Backstepping results in 
\begin{equation*}
u = -e_1^2 - 2e_1^2(e_2 - e_1^2) - k_2e_2, \ k_2 > 0 \Rightarrow (e_1,e_2) \to (0,0), \ (x_1, x_2) \to (0,0)
\end{equation*}
\end{Example}

Generalization-1

\begin{equation*}
\dot{x_1} = f_1(x_1) + g_1(x_1)x_2
\end{equation*}
\begin{equation*}
\dot{x_2} = f_2(x_1, x_2) + g_2(x_1,x_2)u
\end{equation*}

Assumption: $g_2(x_1, x_2) \neq 0 \forall x_1, x_2 \Rightarrow $ Input transformation: $u = \frac{1}{g_2(x_1, x_2)}(V - f_2(x_1, x_2)) \Rightarrow \dot{x_1} = f_1(x_1) + g_1(x_1)x_2 \ \dot{x_2} = V \Rightarrow $ can apply integrator backstepping to determine $V$ results in 

\begin{equation*}
u = \alpha_2(x_1, x_2) = \frac{1}{g_2(x_1,x_2)}(-\frac{\partial V_1}{\partial x_1}g_1(x_1) + \dot{\alpha_1} - k_2(x_2 - \alpha_1(x_1)) - f_2(x_1, x_2))
\end{equation*} 


\underline{Generalization 2:}
(Backstepping through 2 integrators)

$$\dot x_1 = f_1(x_1)+g_1(x_1)x_2, \ \ x_1 \in R^{n_1}$$
$$\dot x_2 = f_2(x_1,x_2)+g_2(x_1,x_2)x_3, \ \ x_2,x_3 \in R$$
$$\dot x_3 = f_3(x_1,x_2,x_3)+g_3(x_1,x_2,x_3)u, \ \ u \in R$$

Assumption: $g_2,g_3$ nowhere zero.\\

Shown before: $\exists \alpha_2$: for $x_3 = \alpha_2(x_1,x_2) \ \ (e_1,e_2) \rightarrow 0$ \\
Thus $e_3 := x_3-\alpha_2(x_1,x_2)$\\

Input transformation:
$$u = \frac{1}{g_3(x_1,x_2,x_3)}(V-f_3(x_1,x_2,x_3))$$
$\implies \dot x_1 = \dots, \dot x_2 = \dots, \dot x_3 = V \implies$ can apply backstepping once more.

In "error" coordinates:
$$\dot e_1 = f_1(e_1)+g_1(e_1)(e_2+\alpha_1(e_1)$$
$$\dot e_2 = f_2(e_1,e_2+\alpha_1(e_1))+g_2(e_1,e_2+\alpha_1(e_1))(e_3+\alpha_2(e_1,e_2+\alpha_1(e_1)))- \dot \alpha_1$$
$$\dot e_3 = V-\dot \alpha_2$$

Define $V_3(e_3) = \frac{1}{2}e_3^2, V(e_1,e_2,e_3) = \sum \limits_{i=1}^3 V_i(e_i)$ \\
$\dot V(e_1,e_2,e_3) = \underline{\frac{\partial V_1}{\partial e_1}(f_1(e_1)+g_1(e_1)(e_2+\alpha_1(e_1)) + e_2(f_2(e_1,e_2+\alpha_1(e_1))+g_2(e_1,e_2+\alpha_1(e_1))}(e_3+ \underline{\alpha_2(e_1,e_2+\alpha_1(e_1)))- \dot \alpha_1)}+e_3(V-\dot \alpha_2)$

All the underlined terms were designed (previously) to be $=L_{f_1+g_1\alpha_1}V_1(e_1)-k^2e_2^2 < 0$

So:
$\dot V(e_1,e_2,e_3) = L_{f_1+g_1\alpha_1}V_1(e_1)-k^2e_2^2 + e_2g_2(e_1,e_2+\alpha_1(e_1))e_3+e_3(V-\dot \alpha_2)$

Structurally it is exactly the same as it was in backstepping through 1.

Choose:
$$V = \dot \alpha_2 - e_2g_2(e_1,e_2+\alpha_1(e_1))-k_3e_3$$
$\dot \alpha_2 - e_2g_2(e_1,e_2+\alpha_1(e_1))$ - "cancelling terms".\\
$k_3e_3$ - "stabilizing term".\\

In original coordinates:\\
$$u=\frac{1}{g_3(x_1,x_2,x_3)}(\dot \alpha_2 - (x_2- \alpha_1(x_1))g_2(x_1,x_2)-k_3(x_3- \alpha_2(x_1,x_2))-f_3(x_1,x_2,x_3))$$
We need $\alpha_1, \alpha_2$ to compute $u$.

\underline{General backstepping recursion:}\\
Systems in "strict feedback form":\\
$\dot x_1 = f_1(x_1)+g_1(x_1)x_2, \ \ x_1 \in R^{n_1}$\\
$\dot x_2 = f_2(x_1,x_2)+g_2(x_1,x_2)x_3$\\
$\dots$\\
$\dot x_k = f_k(x_1,\dots x_k)+g_k(x_1,\dots x_k)u, \ \ x_2, \dots x_k, u \in R$\\
$g_2, \dots g_k$ nowhere zero, $f_i,g_i$ (sufficiantly) smooth, as it is needed in $\alpha_i$.

%TODO Picture

Backstepping recursion:
\begin{enumerate}
    \item "Input data": a CLF $V_1$ for $\dot x_1 = f_1(x_1)+g_1(x_1)u_1$ with a (smooth) feedback $u_1=\alpha_1x_1$ which as. stabilizes the origin of $\dot x_1 = f_1(x_1)+g_1(x_1)u_1$.
    \item for $i=2, \dots k$:\\
    construct a CLF $V_i(e_i)=\frac{1}{2}e_i^2, V = \sum \limits_{j=1}^iV_j(e_j)$ and a feedback $\alpha_1$ which as. stabilizes origin of $(e_1, \dots e_i) = (x_1,x_2-\alpha_1(x_1), \dots, x_i - \alpha_{i-1}(x_1, \dots x_{i-1}))$
    $$\alpha_i(x_1, \dots x_i) = \frac{1}{g_i}(\dot \alpha_{i-1} - \frac{\partial V_{i-1}}{\partial e_{i-1}}g_{i-1}-k_i(x_i-\alpha_{i-1}-f_i)$$
    \item apply $u=\alpha_k(x_1, \dots x_k)$
\end{enumerate}

Backstepping and CLFs:\\
Backstepping is sensitive to uncertainties in $f_i,g_i$ (due to cancelling terms)\\
$\implies$  Sontag's formula is more practical $\implies$ we can use it since $V$ is CLF. \\

Error system is input affine (using input transformation)\\
$\dot e = f(e)+g(e)V$\\
with $f(e) = \begin{pmatrix}
            f_1(e_1)+g_1(e_1)(e_2+\alpha_1(e_1)) \\
            \dots \\
            - \alpha_{k-1}
            \end{pmatrix}, 
    g(e) =\begin{pmatrix}
            0 \\
            0 \\
            \dots \\
            0 \\
            1
            \end{pmatrix}$\\

Claim:\\
$V(e) = \sum \limits_{i=1}^k V_i(e_i)$ is a CLF.
\begin{proof}
    For input affine systems we need to show $L_gV = 0 \implies L_fV < 0, \ \ \forall e \neq 0$.\\
    $\dot V(e) = L_{f_1+g_1\alpha_1}V_1(e_1) - \sum \limits_{i=2}^{k-1}k_ie_i^2 + e_{k-1}g_{k-1}(\dots)e_k - e_k \dot \alpha_{k-1} + e_ku.$\\
    Here $e_ku = L_gV$ and the rest is $L_fV$.\\
    Assume $L_gV = 0 \iff e_k = 0$\\
    $\implies L_fV = L_{f_1+g_1\alpha_1}V_1(e_1) - \sum \limits_{i=2}^{k-1}k_ie_i^2 < 0 \ \ \forall e \neq 0$ with $e_k = 0$.
\end{proof}

$\implies$ We can apply Sontag's formula to construct $V$.\\

This theory can be extended to systems with $x_2, \dots x_k,u \in R^m$ ("block backstepping").