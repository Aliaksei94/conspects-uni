\section{Backstepping}

These remarks from the last lecture, so should be added to the last chapter

\begin{equation*}
\forall x \neq 0: \ \inf_{u} \frac{\partial V}{\partial x} f(x,u) < 0 \ \dot{x} = f(x) + G(x)u
\end{equation*}
So this leads to
\begin{equation*}
\forall x \neq 0 L_G V(x) = 0 \Rightarrow L_fV(x) \neq 0
\end{equation*}
Remark: The last formula is "optimal" if minimize:
\begin{equation*}
\int_{0}^{\infty}\frac{1}{2}p(x)b(x)^Tb(x) + \frac{1}{2p(x)}u^Tudt
\end{equation*}
$b(x) := (L_GV(x))^T$

where $c > 0$

\begin{equation*}
p(x) = \left \{ 
\begin{tabular}{cc} 
$c + \frac{a(x) + \sqrt{a(x)^2 + (b(x)^Tb(x))^2}}{b(x)^Tb(x)}$ & $b(x) \neq 0$ \\ 
$c$ & $b(x) = 0$ 
\end{tabular} 
\right.
\end{equation*}

It still works if $u = \lambda h(x)$ with $\lambda \in [\frac{1}{2}; \infty)$ is applied (large "gain margin")

This is Backstepping

Integrator backstepping 
\begin{equation}\label{system_backstepping}
\dot x_1 = f_1(x_1) + g_1(x_1)x_2 
\end{equation}
\begin{equation*}
\dot x_2 = u
\end{equation*}

where $x_1 \in \mathbb{R}^m, \ x_2, \ u \in \mathbb{R}$ (single input)

image to be inserted

Assumption: we know (smooth) "feedback" $\alpha_1: \mathbb{R}^n \to \mathbb{R}$, and positive definite, differentiable $v_1: \mathbb{R}^m \to \mathbb{R}$

s.t. $L_{f_1 + g_1\alpha_1}V_1(x)$ is negative definite $\Rightarrow$ origin of $\dot{x_1} = f_1(x_1) + g_1(x_1)\alpha_1(x_1)$ is asymptotically stable

Goal: Compute feedback $u = k(x)$ which stabilises (\ref{system_backstepping}). Backstepping constructs $u = \alpha_2(x_1, x_2)$ s.t. $(e_1, e_2) = (x_1 - 0, x_2 - \alpha_1(x_1))=0$ error coordinates

Rewrite (\ref{system_backstepping}) :
\begin{equation*}
\dot x_1 = f_1(x_1) + g_1\alpha_1(x_1) + g_1(x_1)(x_2 - \alpha_1(x_1))
\end{equation*}  
\begin{equation*}
\dot x_2 = u
\end{equation*}

image to be inserted

In error coordinates

\begin{equation}\label{error_coordinates_rewriting}
\dot e_1 = f_1(e_1) + g_1(e_1)\alpha_1(e_1) +g_1(e_1)e_2
\end{equation}
\begin{equation*}
\dot e_2 = u - \dot{\alpha_1} = u - \frac{\partial \alpha_1}{\partial e_1}\dot{e_1} = u - \frac{\partial \alpha_1}{\partial e_1}
\end{equation*}

"backstepping" $\alpha_1$ through the integrator

Define $V_2(e_2):= \frac{1}{2} e_2^2$, and 
\begin{equation*}
V(e_1, e_2) = V_1(e_1) + V_2(e_2)
\end{equation*}  
\begin{equation*}
\dot V(e_1, e_2) = \frac{\partial V_1}{\partial e_1}(f_1(e_1) + g_1(e_1)\alpha_1(e_1)) + \frac{\partial V_1}{\partial e_1}g_1(e_1)e_2 + \frac{\partial V_2}{\partial e_2}(u - \dot{\alpha_1})
\end{equation*}

as far as $L_{f_1 + g_1\alpha_1}V_1$ -negative definite and $\frac{\partial V_2}{\partial e_2} \rightarrow e_2$

Choose 
\begin{equation}\label{u_choice}
u = (- \frac{\partial V_1}{\partial e_1}g_1(e_1) + \dot{\alpha_1})("canaling\ terms") - k_2e_2 ("stabilizing\ term") k_2 > 0
\end{equation}

$\Rightarrow$ Then $\dot{V}(e_1, e_2) = L_{f_1 + g_1\alpha_1}V_1(e_1) - k_2e_2^2 < 0, \ \forall (e_1, e_2) \neq 0$

$\Rightarrow$ Then $\dot V (e_1, e_2) = L_{f_1 + g_1\alpha_1}V_1(e_1) - k_2e_2^2 < 0 \ \forall (e_1, e_2) \neq 0$

$\Rightarrow (e_1, e_2) = (0,0)$ is an asymptotically stable EP for (\ref{error_coordinates_rewriting}) with $u$ as in (\ref{u_choice}) 

Remark: $(e_1, e_2) \to (0,0)$ doesnot necessarily imply that $(x_1, x_2) \to 0$ for $u = \alpha_2(x_1, x_2) = - \frac{\partial V_1}{\partial x_1}g_1(x_1) + \dot{\alpha_1} - k_2(x_2 - \alpha_1(x_1))$

where $u \leftarrow$ (\ref{u_choice}) the original coordinates and $\dot{\alpha_1} \leftarrow \frac{\partial \alpha_1}{\partial x_1}(f_1(x_1) + g_1(x_1)x_2)$

But $(x_1, x_2) = (0,0)$ is asymptotically stable if $\alpha_1(0) = 0$ why? $(e_1,e_2) \to 0 \Rightarrow x_1 \to 0 \ x_2 \to \alpha_1(0) = 0$

\begin{Example}
\begin{equation*}
\dot{x_1} = x_1x_2
\end{equation*}
\begin{equation*}
\dot{x_2} = u
\end{equation*}

Choose $\alpha_1(x_1) = -k$ ($k > 0$) $\rightarrow \dot{x_1} = -k x_1 \Rightarrow V_1(x_1) = \frac{1}{2}x_1^2$

Then: 
\begin{equation*}
e_1 = x_1 - 0 \ \dot{e_1} = e_1(e_2 - k)
\end{equation*}
\begin{equation*}
e_2 = x_2 + k \ e_2 = u
\end{equation*}

Backstepping yields: $u = - e_1^2 - k_2e_2 \ k_2 > 0 \Rightarrow (e_1,e_2) = (0,0)$ is asymptotically stabilized

$(x_1, x_2) = (0,-k)$ is asymptotically stabilized

Can we choose different $\alpha_1$ s.t. $(x_1, x_2) = (0,0)$ is stabilized?

Yes, e.g.
\begin{equation*}
\alpha_1(x_1) = -x_1^2 \Rightarrow \dot{x_1} = - x_1^3 \ V_1(x_1) = \frac{1}{2}x_1^2
\end{equation*}

So we have equations

\begin{equation*}
e_1 = x_1 - 0 \ \dot{e_1}  = e_1(e_2 - e_1^2) 
\end{equation*}
\begin{equation*}
e_2 = x_2 + x_1^2 \ \dot{e_2} = u + 2e_1^2(e_2 - e_1^2)
\end{equation*}

Backstepping results in 
\begin{equation*}
u = -e_1^2 - 2e_1^2(e_2 - e_1^2) - k_2e_2, \ k_2 > 0 \Rightarrow (e_1,e_2) \to (0,0), \ (x_1, x_2) \to (0,0)
\end{equation*}
\end{Example}

Generalization-1

\begin{equation*}
\dot{x_1} = f_1(x_1) + g_1(x_1)x_2
\end{equation*}
\begin{equation*}
\dot{x_2} = f_2(x_1, x_2) + g_2(x_1,x_2)u
\end{equation*}

Assumption: $g_2(x_1, x_2) \neq 0 \forall x_1, x_2 \Rightarrow $ Input transformation: $u = \frac{1}{g_2(x_1, x_2)}(V - f_2(x_1, x_2)) \Rightarrow \dot{x_1} = f_1(x_1) + g_1(x_1)x_2 \ \dot{x_2} = V \Rightarrow $ can apply integrator backstepping to determine $V$ results in 

\begin{equation*}
u = \alpha_2(x_1, x_2) = \frac{1}{g_2(x_1,x_2)}(-\frac{\partial V_1}{\partial x_1}g_1(x_1) + \dot{\alpha_1} - k_2(x_2 - \alpha_1(x_1)) - f_2(x_1, x_2))
\end{equation*} 

\section{Practice Exercise 3}

Motivation: Lyapunov Theory

\begin{equation*}
\dot{x} = f(x,u)
\end{equation*}
$f:\mathbb{R}^n \times \mathbb{R}^m \to \mathbb{R}^n$

\begin{Definition}
(CLF) A function $V: \mathbb{R}^n \to \mathbb{R}$ is a CLF if it is continuous differentiable, positive definite, radially unbounded and $ \forall x \neq 0 \ \inf_{u}< \triangledown V(x), f(x,u) > < 0$ 
\end{Definition}

In order to find CLFs, we restrict our analysis to input -affine systems
\begin{equation*}
\dot{x} = f(x) + G(x)u
\end{equation*}
where $f: \mathbb{R}^n \to \mathbb{R}^n, \ G: \mathbb{R}^n \to \mathbb{R}^{n \times m}$

Proposition: A continuous, differentiable, positive definite and radially unbounded. $V: \mathbb{R}^n \to \mathbb{R}$ is a CLF iff 
\begin{equation*}
\forall x \neq 0 \ L_GV(x) = 0 \Rightarrow L_fV(x) < 0
\end{equation*}

Image to be inserted

Problem 1

Consider $\dot{x} = cos(x) + (1+e^x)u$ where $f(x) = cos(x)$- drift and $g(x) = 1+e^x$

Let $V: \mathbb{R} \to \mathbb{R}, \ x \mapsto \frac{1}{2}x^2$. Clearly, continuous differentiable, positive definite and radially unbounded. Moreover, for any nonzero $x$, we have $L_GV(x) \neq 0$. 

Thus, for any $x \neq 0$, there exists a control that readers $<\triangledown V(x), f(x) + g(x)u>$ negative. 
Givn this CLF, there exists a state feedback $u = u(x)$, e.g. 
\begin{equation*}
u(x) = - \frac{kx+cos(x)}{1+e^x}, \ k > 0
\end{equation*}

Problem

Consider 
\begin{equation*}
\dot{x_1} = -x_1^3 + x_2e^{x_1}cos(x_2)
\end{equation*}  
\begin{equation*}
\dot{x_2} = x_1^5sin(x_2) + u
\end{equation*}

Take $V: \mathbb{R}^2 \to \mathbb{R}, \ (x_1, x_2) \mapsto \frac{1}{2}(x_1^2 + x_2^2)$

For any $x \neq 0$, we have 
\begin{equation*}
\inf_{u \in \mathbb{R}}(L_fV(x) + L_GV(x)u) = 
\left \{ 
\begin{tabular}{cc} 
$L_fV(x), \ $ & $if\ L_GV(x) = 0$ \\ 
$- \infty$ & $else$ 
\end{tabular} 
\right.
\end{equation*}

In particular,
\begin{equation*}
L_fV(x) = \dots = x_1(-x_1^3 + x_2e^x_1 cos(x_2)) + x_2x_1^5sin(x_2)
\end{equation*}
\begin{equation*}
L_GV(x) = \dots = x_2
\end{equation*}

However, 
\begin{equation*}
L_fV(x)|_{x_2 = 0} = -x_1^4 < 0 \ \forall x_1 \neq 0
\end{equation*}

Image to be inserted

Concluding that $V$ is a CLF.

Problem 2:

$\dot{x} = Ax + Bu$, input defined system where $(A,B)$ is stabilizable, there exists $K \in \mathbb{R}^{m \times n}$ s.t. $A+BK$ is Hurwitz (cf. KRT).The latter is equivalent to the existance $P = P^T > 0$ s.t. $P(A+BK) + (A+BK)^TP < 0$ (cf. Khalil theorem 4,6)

Let $V: \mathbb{R}^n \to \mathbb{R}, x \mapsto <x, Px>$. Moreover, $\forall x \neq 0 \exists u = Kx$ s.t. $<\triangledown V(x), Ax+Bu> < 0$, since 
\begin{equation*}
<\triangledown V(x), Ax+Bu> =^{u = Kx} <x, (P(A+BK)+ (A+BK)^TP)x> < 0
\end{equation*} 

In addition,
\begin{equation*}
\forall \epsilon > 0 \exists \delta = \frac{\epsilon}{\|K\| } > 0 \ \forall x \neq 0, \ \|x\| < \delta \ \exists u = Kx \ \|u\| < \epsilon 
\end{equation*}
s.t. $L_fV(x) + L_GV(x)u < 0$ since $\|u\| = \|Kx\| \leq \|K\|\|x\| < \|K\|\delta = \epsilon$

Problem 3

Let $P: \mathbb{R}^2 \to \mathbb{R}$ be continuous, differentiable consider 
\begin{equation*}
m\dot{v}  = - g \triangledown P(q) + F, \ m,g >0
\end{equation*} 
a) Hamiltonian form. Let $x:=(q,v)$. Then $\dot{x} = (-\frac{g}{m}\triangledown P(q) + \frac{1}{m}F)= \begin{bmatrix}
 & I \\
 -I & 
\end{bmatrix}\begin{bmatrix}
 \frac{g}{m}\triangledown P(q) \\
 V 
\end{bmatrix} + \begin{bmatrix}
  \\
 \frac{1}{m}I
\end{bmatrix}F = \begin{bmatrix}
 & I \\
 -I & 
\end{bmatrix} \triangledown H(x) + G(x)$ given $H(x) = \frac{1}{2}\|\nu\|^2 + \frac{g}{m}P(q)$

b) "CLF". Take $H$ as a CLF candidate. Then, for any $x$ 
\begin{equation*}
<\triangledown H(x), \begin{bmatrix}
 & I \\
 -I & 
\end{bmatrix} \triangledown H(x) + G(x)F> = <\triangledown H(x), \begin{bmatrix}
 & I \\
 -I & 
\end{bmatrix} \triangledown H(x)> + <\triangledown H(x), G(x)F> = [<\triangledown H(x), \begin{bmatrix}
 & I \\
 -I & 
\end{bmatrix} \triangledown H(x)> = L_fH(x) = 0] = \frac{1}{m} <\nu, F>
\end{equation*}

Strictly speaking, $H$ is no CLF, but it reveals how to choose $F$ s.t. the origin is GAS.

For any point $x$ for which there exists no control $F$ s.t. $<\triangledown H(x), \begin{bmatrix}
 & I \\
 -I & 
\end{bmatrix} \triangledown H(x) + G(x)F> < 0$

Choose $F = 0$. Why? Using the Krasovsky-Lasallle inv. principle, we conclude that the origin is GAS, since any solution in $\{ x| \dot{H}(x) = 0 \}$ verifies $v(t) \equiv 0$, implying $\dot{v}(t) \equiv 0$ s.t. 
\begin{equation*}
0 = - \frac{g}{m} \triangledown P(q(t)) + \frac{1}{m} P(t)
\end{equation*}
The last part equals 0.  Since $F = 0$ (by choice) and $\triangledown P(q) = 0$ iff $q = 0$ we conclude that $\dot{H}(x) = 0$ can only be "maintained" at the origin.

Problem 4

Consider 

\begin{equation*}
\dot{x_1} = x_2
\end{equation*}
\begin{equation*}
\dot{x_2} = - ux_2 + u^3
\end{equation*}

show that $V(x) = \frac{1}{2} x_1^2 + \frac{1}{2}(x_1 +x_2)^2$ is CLF and let $V: \mathbb{R}^n \to \mathbb{R}$ be defined by

\begin{equation*}
\ddot{x} + u\dot{x} - u^3 = 0
\end{equation*}

For any $x$ and $u$, we have $<\triangledown V(x), f(x,u)> = \dots = x_1(2x_2 -ux_2 + u^3) + x_2(x_2 - ux_2 + u^3) = x_1h_1 + x_2h_2$

Image to be inserted

Hence if $u < 0$ and $-u$ "large", then we can render $<\triangledown V(x), f(x,u)> < 0$.
